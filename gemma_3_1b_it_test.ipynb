{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Sz9yfAjEelenELysPhPIDMduaD73m7wq",
      "authorship_tag": "ABX9TyO8zXpZ8hLXuB8jAxAqIE+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prikshitkverma/Gemma_fine_tuning/blob/main/gemma_3_1b_it_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKm1lpygECla"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1. INSTALL LIBRARIES\n",
        "# ============================================\n",
        "#!pip uninstall -y torch torchvision torchaudio transformers trl accelerate datasets huggingface_hub\n",
        "#!pip install -q torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "#!pip install -q transformers==4.45.2 trl==0.11.6 accelerate==1.1.1 datasets==3.1.0 huggingface_hub==0.28.1 sentencepiece pyarrow==18.0.0 evaluate tensorboard\n",
        "!pip uninstall -y torch torchvision torchaudio transformers trl accelerate datasets huggingface_hub\n",
        "!pip install -q torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q transformers==4.45.2 trl==0.11.6 accelerate==1.1.1 datasets==3.1.0 huggingface_hub==0.28.1 sentencepiece pyarrow==18.0.0\n",
        "!pip install -q datasets trl sentencepiece huggingface_hub\n",
        "!pip install evaluate\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 2. SETUP AND AUTHENTICATION\n",
        "# ============================================\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from huggingface_hub import login\n",
        "import evaluate\n",
        "\n",
        "# Login to Hugging Face\n",
        "HF_TOKEN = \"hf_token\"\n",
        "login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "So9cbUZQGa2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 3. CONFIGURE MODEL AND DIRECTORIES\n",
        "# ============================================\n",
        "base_model = \"google/gemma-3-1b-it\"\n",
        "output_dir = \"./gemma-natural-farming-qa\"\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 4. LOAD AND PREPARE THE DATASET\n",
        "# ============================================\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_file = \"/content/nf_datasett.jsonl\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=data_file,\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# Shuffle\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# 80% train, 10% val, 10% test\n",
        "train_test = dataset.train_test_split(test_size=0.2)\n",
        "val_test = train_test[\"test\"].train_test_split(test_size=0.5)\n",
        "\n",
        "dataset_dict = {\n",
        "    \"train\": train_test[\"train\"],\n",
        "    \"validation\": val_test[\"train\"],\n",
        "    \"test\": val_test[\"test\"]\n",
        "}\n",
        "\n",
        "# Inspect dataset\n",
        "print(\"‚úÖ Dataset Split Summary:\")\n",
        "print(\n",
        "    f\"Train: {len(dataset_dict['train'])} | \"\n",
        "    f\"Validation: {len(dataset_dict['validation'])} | \"\n",
        "    f\"Test: {len(dataset_dict['test'])}\"\n",
        ")\n",
        "\n",
        "print(\"\\nExample data sample:\")\n",
        "print(dataset_dict[\"train\"][0][\"messages\"])\n"
      ],
      "metadata": {
        "id": "pwMkqf_RE0qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 5. LOAD MODEL AND TOKENIZER\n",
        "# ============================================\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "print(f\"‚úÖ Model loaded on {model.device} | dtype: {model.dtype}\")"
      ],
      "metadata": {
        "id": "CQdMunFKEdbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 6. CONFIGURE THE TRAINING PROCESS\n",
        "# ============================================\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    # max_seq_length=256, # Removed max_seq_length\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    fp16=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "tG-S2XNFEhKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 7. TRAIN AND VALIDATE MODEL\n",
        "# ============================================\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset_dict[\"train\"],\n",
        "    eval_dataset=dataset_dict[\"validation\"],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"üíæ Saving final model...\")\n",
        "trainer.save_model(output_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "PncOhlO3E7Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "_MCj_YalTG7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# 8. VALIDATE MODEL PERFORMANCE\n",
        "# ============================================\n",
        "print(\"\\nüîç Validating model on validation set...\")\n",
        "model.eval()\n",
        "\n",
        "# Use a simple text-generation pipeline\n",
        "val_pipe = pipeline(\"text-generation\", model=output_dir, tokenizer=tokenizer)\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "generated_texts = []\n",
        "reference_texts = []\n",
        "\n",
        "for i, sample in enumerate(dataset_dict[\"validation\"]):\n",
        "    user_msg = [{\"role\": \"user\", \"content\": sample[\"messages\"][0][\"content\"]}]\n",
        "    prompt = tokenizer.apply_chat_template(user_msg, tokenize=False, add_generation_prompt=True)\n",
        "    output = val_pipe(prompt, max_new_tokens=128, num_return_sequences=1)[0][\"generated_text\"][len(prompt):].strip()\n",
        "    generated_texts.append(output)\n",
        "    reference_texts.append(sample[\"messages\"][1][\"content\"])\n",
        "    if i < 3:\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Q: {sample['messages'][0]['content']}\")\n",
        "        print(f\"Model: {output}\")\n",
        "        print(f\"Ref: {sample['messages'][1]['content']}\")\n",
        "\n",
        "# Compute metrics\n",
        "bleu_score = bleu.compute(predictions=generated_texts, references=reference_texts)\n",
        "rouge_score = rouge.compute(predictions=generated_texts, references=reference_texts)\n",
        "\n",
        "print(\"\\nüìä Validation Metrics:\")\n",
        "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_score['rougeL']:.4f}\")"
      ],
      "metadata": {
        "id": "j-2sOtXpE8U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install bert_score\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "results = bertscore.compute(predictions=generated_texts, references=reference_texts, lang=\"en\")\n",
        "print(sum(results[\"f1\"]) / len(results[\"f1\"]))"
      ],
      "metadata": {
        "id": "KDEym4z0FAce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_perplexity(model, tokenizer, dataset):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    for sample in dataset:\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            sample[\"messages\"],\n",
        "            tokenize=False\n",
        "        )\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            losses.append(outputs.loss.item())\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    return math.exp(mean_loss)\n",
        "\n",
        "ppl = compute_perplexity(model, tokenizer, dataset_dict[\"validation\"])\n",
        "print(f\"\\nüìâ Validation Perplexity: {ppl:.2f}\")\n"
      ],
      "metadata": {
        "id": "0lEHyfltL2c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 9. TEST INTERACTIVELY\n",
        "# ============================================\n",
        "print(\"\\n--- Interactive Testing ---\")\n",
        "test_pipe = pipeline(\"text-generation\", model=output_dir, tokenizer=tokenizer)\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nEnter your question (or type 'exit' to quit): \").strip()\n",
        "    if question.lower() == \"exit\":\n",
        "        print(\"üëã Exiting...\")\n",
        "        break\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    outputs = test_pipe(prompt, max_new_tokens=256)\n",
        "    print(f\"\\nüß† Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
        ""
      ],
      "metadata": {
        "id": "UqXlSXcRFDlv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}